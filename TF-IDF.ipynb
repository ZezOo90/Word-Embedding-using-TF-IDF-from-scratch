{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "PnYtoRuR8PsU"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBj2aaDR89zs",
        "outputId": "83aeb1fc-a2a0-42be-b522-413dd2e78c15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0uAM1gzD6_U",
        "outputId": "c5117a6d-bfce-4337-db4b-df227c442946"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "docs =[]\n",
        "topics = [\"supervised learning\",\"unsupervised learning\",\"Reinforcement learning\"]\n",
        "for topic in topics:\n",
        "  # Initialize the text generation pipeline with the desired model\n",
        "  generator = pipeline('text-generation', model='gpt2')\n",
        "  prompt = topic\n",
        "  generated_text = generator(topic, max_length=50)\n",
        "  docs.append(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI6tU8JvFOL9",
        "outputId": "3f82c48b-0ba2-44e1-d19f-8848a0d0c424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[{'generated_text': 'supervised learning for high-performance computing. Based on this understanding of the theoretical and practical applications of neural networks, we propose a computer operating system that can implement both network security and deep learning to enhance the security of computerized AI systems.\\n\\n'}], [{'generated_text': 'unsupervised learning, a method that might provide better information on future trends in artificial intelligence.\\n\\nAnd, even with that potential, the project is still under an aggressive development stage, with more questions than answers from what the algorithms can learn and'}], [{'generated_text': \"Reinforcement learning is a key driver of a large social network, meaning that a strong interest in learning is essential as we build an effective social network. We've been thinking about reinstating reinforcement learning while exploring the concept of neural-machine learning\"}]]\n"
          ]
        }
      ],
      "source": [
        "print(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANmVD9_yM_uG",
        "outputId": "fe65c1bb-c0c1-4228-f2f4-62c92c1db363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['youll', 'learning', 'here', 'ever', 'supervised', 'article', 'ive', 'algorithm', 'complicated', 'problem', 'interesting', 'wrote', 'often', 'think', 'would', 'solve', 'example', 'ask', 'also', 'one']\n"
          ]
        }
      ],
      "source": [
        "# Apply cleaning function to each document\n",
        "unique_words = []\n",
        "documents_=[]\n",
        "for doc in docs:\n",
        "    for sub_doc in doc:\n",
        "        for key, value in sub_doc.items():\n",
        "            # Normalization\n",
        "            cleaned_data = re.sub(r'[^a-zA-Z\\s]', '', value)\n",
        "            # Tokonization\n",
        "            tokonizes = nltk.word_tokenize(cleaned_data.lower())\n",
        "            # Remove stopwords\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            tokens = [word for word in tokonizes if word not in stop_words]\n",
        "            # lemmatizing\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            lemmatizer_words = [lemmatizer.lemmatize(word)for word in tokens]\n",
        "            #unique words\n",
        "            unique_words.append(list(set(lemmatizer_words)))\n",
        "            documents_.append(lemmatizer_words)\n",
        "\n",
        "print(unique_words[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bDbSwmT2Lnp",
        "outputId": "c4b8ba5c-2252-43a4-f3d2-41ced1e64fa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.20933872 0.20933872 0.20933872 0.20933872 0.         0.20933872\n",
            "  0.         0.         0.20933872 0.31841498 0.         0.\n",
            "  0.20933872 0.         0.20933872 0.20933872 0.         0.\n",
            "  0.         0.12363882 0.         0.         0.         0.20933872\n",
            "  0.15920749 0.         0.41867744 0.         0.         0.\n",
            "  0.20933872 0.         0.15920749 0.         0.20933872 0.\n",
            "  0.         0.         0.         0.         0.         0.20933872\n",
            "  0.20933872 0.20933872]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.23652731 0.23652731 0.         0.17988511 0.23652731 0.\n",
            "  0.         0.23652731 0.         0.         0.         0.\n",
            "  0.         0.41909052 0.23652731 0.         0.         0.\n",
            "  0.         0.         0.         0.17988511 0.23652731 0.47305462\n",
            "  0.         0.         0.17988511 0.         0.         0.\n",
            "  0.         0.23652731 0.23652731 0.23652731 0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.17815825 0.\n",
            "  0.         0.         0.         0.         0.         0.3563165\n",
            "  0.         0.         0.         0.         0.17815825 0.3563165\n",
            "  0.17815825 0.10522313 0.         0.3563165  0.17815825 0.\n",
            "  0.13549394 0.17815825 0.         0.13549394 0.         0.\n",
            "  0.         0.53447475 0.         0.17815825 0.         0.17815825\n",
            "  0.17815825 0.         0.         0.         0.17815825 0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import numpy as np\n",
        "\n",
        "corpus=[' '.join(doc) for doc in documents_]\n",
        "\n",
        "# Compute TF for each word\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get vocabulary\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "#Calculate IDF for each word\n",
        "transformer = TfidfTransformer()\n",
        "transformer.fit(X)\n",
        "\n",
        "# Get IDF values\n",
        "idf_values = transformer.idf_\n",
        "\n",
        "#Compute TF * IDF\n",
        "tfidf_matrix = np.zeros((len(corpus), len(vocab)))\n",
        "for i in range(len(corpus)):\n",
        "    for j, word in enumerate(vocab):\n",
        "        tfidf_matrix[i][j] = X[i, vectorizer.vocabulary_.get(word)] * idf_values[vectorizer.vocabulary_.get(word)]\n",
        "\n",
        "# Normalize TF-IDF for each document\n",
        "normalized_tfidf_matrix = np.zeros_like(tfidf_matrix)\n",
        "\n",
        "for i in range(len(corpus)):\n",
        "    tfidf_sum_squares = np.sqrt(np.sum(np.square(tfidf_matrix[i])))\n",
        "    normalized_tfidf_matrix[i] = tfidf_matrix[i] / tfidf_sum_squares\n",
        "\n",
        "print(normalized_tfidf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLkhyr69uUdA"
      },
      "source": [
        "# **From scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-59vmgOawD7",
        "outputId": "0a7f2034-7408-4f7f-aa06-04c1ebcf20eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.20933872 0.20933872 0.20933872 0.20933872 0.         0.20933872\n",
            "  0.         0.         0.20933872 0.31841498 0.         0.\n",
            "  0.20933872 0.         0.20933872 0.20933872 0.         0.\n",
            "  0.         0.12363882 0.         0.         0.         0.20933872\n",
            "  0.15920749 0.         0.41867744 0.         0.         0.\n",
            "  0.20933872 0.         0.15920749 0.         0.20933872 0.\n",
            "  0.         0.         0.         0.         0.         0.20933872\n",
            "  0.20933872 0.20933872]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.23652731 0.23652731 0.         0.17988511 0.23652731 0.\n",
            "  0.         0.23652731 0.         0.         0.         0.\n",
            "  0.         0.41909052 0.23652731 0.         0.         0.\n",
            "  0.         0.         0.         0.17988511 0.23652731 0.47305462\n",
            "  0.         0.         0.17988511 0.         0.         0.\n",
            "  0.         0.23652731 0.23652731 0.23652731 0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.18002842 0.\n",
            "  0.         0.         0.         0.         0.         0.36005684\n",
            "  0.         0.         0.         0.         0.18002842 0.36005684\n",
            "  0.10632768 0.10632768 0.         0.36005684 0.18002842 0.\n",
            "  0.13691625 0.18002842 0.         0.13691625 0.         0.\n",
            "  0.         0.54008526 0.         0.18002842 0.         0.18002842\n",
            "  0.18002842 0.         0.         0.         0.18002842 0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def count_vectorizer(documents):\n",
        "\n",
        "    vocab = set()\n",
        "    word_count = []\n",
        "\n",
        "    for doc in documents:\n",
        "        words = doc.split()\n",
        "        word_count_doc = {}\n",
        "        for word in words:\n",
        "            if word not in word_count_doc:\n",
        "                word_count_doc[word] = 1\n",
        "            else:\n",
        "                word_count_doc[word] += 1\n",
        "            vocab.add(word)\n",
        "        word_count.append(word_count_doc)\n",
        "\n",
        "    vocab = list(vocab)\n",
        "    vocab.sort()\n",
        "\n",
        "    return word_count, vocab\n",
        "\n",
        "def compute_idf(documents, vocab):\n",
        "\n",
        "    idf_values = {}\n",
        "\n",
        "    for word in vocab:\n",
        "        doc_count = sum(1 for doc in documents if word in doc)\n",
        "        idf_values[word] = np.log((len(documents)+1) / (1 + doc_count)) +1\n",
        "\n",
        "    return idf_values\n",
        "\n",
        "def compute_tfidf(word_count, idf_values, vocab):\n",
        "\n",
        "    tfidf_matrix = np.zeros((len(word_count), len(vocab)))\n",
        "\n",
        "    for i, word_count_doc in enumerate(word_count):\n",
        "        for j, word in enumerate(vocab):\n",
        "            tf = word_count_doc.get(word, 0)\n",
        "            tfidf_matrix[i][j] = tf * idf_values[word]\n",
        "\n",
        "    return tfidf_matrix\n",
        "\n",
        "def normalize_tfidf(tfidf_matrix):\n",
        "\n",
        "    # Normalize TF-IDF for each document\n",
        "    normalized_tfidf_matrix = np.zeros_like(tfidf_matrix)\n",
        "\n",
        "    for i in range(len(corpus)):\n",
        "         tfidf_sum_squares = np.sqrt(np.sum(np.square(tfidf_matrix[i])))\n",
        "         normalized_tfidf_matrix[i] = tfidf_matrix[i] / tfidf_sum_squares\n",
        "    return normalized_tfidf_matrix\n",
        "\n",
        "\n",
        "#compute TF for each word\n",
        "word_count, vocab = count_vectorizer(corpus)\n",
        "\n",
        "#calculate IDF for each word\n",
        "idf_values = compute_idf(corpus, vocab)\n",
        "\n",
        "# compute TF * IDF\n",
        "tfidf_matrix = compute_tfidf(word_count, idf_values, vocab)\n",
        "\n",
        "#normalize TF-IDF for each document\n",
        "normalized_tfidf_matrix = normalize_tfidf(tfidf_matrix)\n",
        "\n",
        "print(normalized_tfidf_matrix)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
